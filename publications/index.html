<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Marius Mosbach | publications</title>
<meta name="description" content="Academic website of Marius Mosbach. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/css/mdb.min.css" integrity="sha256-/SwJ2GDcEt5382i8zqDwl36VJGECxEoIcBIuoLmLR4g=" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css"  integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- Open Graph -->


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light bg-white navbar-expand-sm fixed-top">
    <div class="container">
      
      
      
      
      
      <!-- <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Marius</span>   Mosbach
      </a>
      --->
      <div class="navbar-brand title font-weight-lighter">
        <span class="font-weight-bold">Marius</span>   Mosbach
      </div>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description">Publications and preprints in reversed chronological order. For an always up-to-date list check my Google Scholar.</p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2023</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">BabyLM 2023</abbr>
      
        <award class="badge">Best paper</award>
      
    
  
  </div>

  <div id="steuer-etal-2023-large" class="col-sm-8">
    
      <span class="title">Large GPT-like Models are Bad Babies: A Closer Look at the Relationship between Linguistic Competence and Psycholinguistic Measures</span>
      <span class="author">
        
          
            
              
                
                  Steuer, Julius,
                
              
            
          
        
          
            
              
                <em>Mosbach, Marius</em>,
              
            
          
        
          
            
              
                
                  and Klakow, Dietrich
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In </em>
      
      
        2023
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Research on the cognitive plausibility of language models (LMs) has so far mostly concentrated on modelling psycholinguistic response variables such as reading times, gaze durations and N400/P600 EEG signals, while mostly leaving out the dimension of what Mahowald et al. (2023) described as formal and functional linguistic competence, and developmental plausibility. We address this gap by training a series of GPT-like language models of different sizes on the strict version of the BabyLM pretraining corpus, evaluating on the challenge tasks (BLiMP, GLUE, MSGS) and an additional reading time prediction task. We find a positive correlation between LM size and performance on all three challenge tasks, with different preferences for model width and depth in each of the tasks. In contrast, a negative correlation was found between LM size and reading time fit of linear mixed-effects models using LM surprisal as a predictor, with the second-smallest LM achieving the largest log-likelihood reduction over a baseline model without surprisal. This suggests that modelling processing effort and linguistic competence may require an approach different from training GPT-like LMs on a developmentally plausible corpus.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL 2023</abbr>
      
    
  
  </div>

  <div id="mosbach-etal-2023-shot" class="col-sm-8">
    
      <span class="title">Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation</span>
      <span class="author">
        
          
            
              
                <em>Mosbach, Marius</em>,
              
            
          
        
          
            
              
                
                  Pimentel, Tiago,
                
              
            
          
        
          
            
              
                
                  Ravfogel, Shauli,
                
              
            
          
        
          
            
              
                
                  Klakow, Dietrich,
                
              
            
          
        
          
            
              
                
                  and Elazar, Yanai
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Findings of the Association for Computational Linguistics: ACL 2023</em>
      
      
        2023
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Few-shot fine-tuning and in-context learning are two alternative strategies for task adaptation of pre-trained language models. Recently, in-context learning has gained popularity over fine-tuning due to its simplicity and improved out-of-domain generalization, and because extensive evidence shows that fine-tuned models pick up on spurious correlations.Unfortunately, previous comparisons of the two approaches were done using models of different sizes. This raises the question of whether the observed weaker out-of-domain generalization of fine-tuned models is an inherent property of fine-tuning or a limitation of the experimental setup. In this paper, we compare the generalization of few-shot fine-tuning and in-context learning to challenge datasets, while controlling for the models used, the number of examples, and the number of parameters, ranging from 125M to 30B. Our results show that fine-tuned language models can in fact generalize well out-of-domain. We find that both approaches generalize similarly; they exhibit large variation and depend on properties such as model size and the number of examples, highlighting that robust task adaptation remains a challenge.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL 2023</abbr>
      
        <award class="badge">Best paper</award>
      
    
  
  </div>

  <div id="zhu-etal-2023-weaker" class="col-sm-8">
    
      <span class="title">Weaker Than You Think: A Critical Look at Weakly Supervised Learning</span>
      <span class="author">
        
          
            
              
                
                  Zhu, Dawei,
                
              
            
          
        
          
            
              
                
                  Shen, Xiaoyu,
                
              
            
          
        
          
            
              
                <em>Mosbach, Marius</em>,
              
            
          
        
          
            
              
                
                  Stephan, Andreas,
                
              
            
          
        
          
            
              
                
                  and Klakow, Dietrich
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
      
      
        2023
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Weakly supervised learning is a popular approach for training machine learning models in low-resource settings. Instead of requesting high-quality yet costly human annotations, it allows training models with noisy annotations obtained from various weak sources. Recently, many sophisticated approaches have been proposed for robust training under label noise, reporting impressive results. In this paper, we revisit the setup of these approaches and find that the benefits brought by these approaches are significantly overestimated. Specifically, we find that the success of existing weakly supervised learning approaches heavily relies on the availability of clean validation samples which, as we show, can be leveraged much more efficiently by simply training on them. After using these clean labels in training, the advantages of using these sophisticated approaches are mostly wiped out. This remains true even when reducing the size of the available clean data to just five samples per class, making these approaches impractical. To understand the true value of weakly supervised learning, we thoroughly analyze diverse NLP datasets and tasks to ascertain when and why weakly supervised approaches work. Based on our findings, we provide recommendations for future research.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">COLING 2022</abbr>
      
        <award class="badge">Best paper</award>
      
    
  
  </div>

  <div id="alabi-etal-2022-adapting" class="col-sm-8">
    
      <span class="title">Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning</span>
      <span class="author">
        
          
            
              
                
                  Alabi, Jesujoba O.,
                
              
            
          
        
          
            
              
                
                  Adelani, David Ifeoluwa,
                
              
            
          
        
          
            
              
                <em>Mosbach, Marius</em>,
              
            
          
        
          
            
              
                
                  and Klakow, Dietrich
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 29th International Conference on Computational Linguistics</em>
      
      
        2022
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Multilingual pre-trained language models (PLMs) have demonstrated impressive performance on several downstream tasks for both high-resourced and low-resourced languages. However, there is still a large performance drop for languages unseen during pre-training, especially African languages. One of the most effective approaches to adapt to a new language is language adaptive fine-tuning (LAFT) — fine-tuning a multilingual PLM on monolingual texts of a language using the pre-training objective. However, adapting to target language individually takes large disk space and limits the cross-lingual transfer abilities of the resulting models because they have been specialized for a single language. In this paper, we perform multilingual adaptive fine-tuning on 17 most-resourced African languages and three other high-resource languages widely spoken on the African continent to encourage cross-lingual transfer learning. To further specialize the multilingual PLM, we removed vocabulary tokens from the embedding layer that corresponds to non-African writing scripts before MAFT, thus reducing the model size by around 50%. Our evaluation on two multilingual PLMs (AfriBERTa and XLM-R) and three NLP tasks (NER, news topic classification, and sentiment classification) shows that our approach is competitive to applying LAFT on individual languages while requiring significantly less disk space. Additionally, we show that our adapted PLM also improves the zero-shot cross-lingual transfer abilities of parameter efficient fine-tuning methods.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NAACL 2022</abbr>
      
    
  
  </div>

  <div id="zhang2022mcse" class="col-sm-8">
    
      <span class="title">MCSE: Multimodal Contrastive Learning of Sentence Embeddings</span>
      <span class="author">
        
          
            
              
                
                  Zhang, Miaoran,
                
              
            
          
        
          
            
              
                <em>Mosbach, Marius</em>,
              
            
          
        
          
            
              
                
                  Adelani, David Ifeoluwa,
                
              
            
          
        
          
            
              
                
                  Hedderich, Michael A.,
                
              
            
          
        
          
            
              
                
                  and Klakow, Dietrich
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>NAACL 2022</em>
      
      
        2022
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/abs/2204.10931" target="_blank">HTML</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Learning semantically meaningful sentence embeddings is an open problem in natural language processing. In this work, we propose a sentence embedding learning approach that exploits both visual and textual information via a multimodal contrastive objective. Through experiments on a variety of semantic textual similarity tasks, we demonstrate that our approach consistently improves the performance across various datasets and pre-trained encoders. In particular, combining a small amount of multimodal data with a large text-only corpus, we improve the state-of-the-art average Spearman’s correlation by 1.7%. By analyzing the properties of the textual embedding space, we show that our model excels in aligning semantically similar sentences, providing an explanation for its improved performance.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SpaNLP 2022</abbr>
      
    
  
  </div>

  <div id="zouhar2021knowledge" class="col-sm-8">
    
      <span class="title">Knowledge Base Index Compression via Dimensionality and Precision Reduction</span>
      <span class="author">
        
          
            
              
                
                  Zouhar, Vilém,
                
              
            
          
        
          
            
              
                <em>Mosbach, Marius</em>,
              
            
          
        
          
            
              
                
                  and Klakow, Dietrich
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>SpaNLP workshop @ ACL 2022</em>
      
      
        2022
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/abs/2204.02906" target="_blank">HTML</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Recently neural network based approaches to knowledge-intensive NLP tasks, such as question answering, started to rely heavily on the combination of neural retrievers and readers. Retrieval is typically performed over a large textual knowledge base (KB) which requires significant memory and compute resources, especially when scaled up. On HotpotQA we systematically investigate reducing the size of the KB index by means of dimensionality (sparse random projections, PCA, autoencoders) and numerical precision reduction. Our results show that PCA is an easy solution that requires very little data and is only slightly worse than autoencoders, which are less stable. All methods are sensitive to pre- and post-processing and data should always be centered and normalized both before and after dimension reduction. Finally, we show that it is possible to combine PCA with using 1bit per dimension. Overall we achieve (1) 100× compression with 75%, and (2) 24× compression with 92% original retrieval performance.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AKBC 2021</abbr>
      
    
  
  </div>

  <div id="zouhar2021artefact" class="col-sm-8">
    
      <span class="title">Artefact Retrieval: Overview of NLP Models with Knowledge Base Access</span>
      <span class="author">
        
          
            
              
                
                  Zouhar, Vilém,
                
              
            
          
        
          
            
              
                <em>Mosbach, Marius</em>,
              
            
          
        
          
            
              
                
                  Biswas, Debanjali,
                
              
            
          
        
          
            
              
                
                  and Klakow, Dietrich
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>CSKB workshop @ AKBC</em>
      
      
        2021
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://openreview.net/forum?id=9_oCNR6R9l2" target="_blank">HTML</a>]
    
    
    
    
    
    
      [<a href="https://github.com/uds-lsv" target="_blank">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Many NLP models gain performance by having access to a knowledge base. A lot of research has been devoted to devising and improving the way the knowledge base is accessed and incorporated into the model, resulting in a number of mechanisms and pipelines. Despite the diversity of proposed mechanisms, there are patterns in the designs of such systems.
In this paper, we systematically describe the typology of *artefacts* (items retrieved from a knowledge base), retrieval mechanisms and the way these artefacts are *fused* into the model. This further allows us to uncover combinations of design decisions that had not yet been tried. Most of the focus is given to language models, though we also show how question answering, fact-checking and knowledgable dialogue models fit into this system as well. Having an abstract model which can describe the architecture of specific models also helps with transferring these architectures between multiple NLP tasks.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Interspeech 2021</abbr>
      
    
  
  </div>

  <div id="abdullah2021do" class="col-sm-8">
    
      <span class="title">Do Acoustic Word Embeddings Capture Phonological Similarity? An Empirical Study</span>
      <span class="author">
        
          
            
              
                
                  Abdullah, Badr M.,
                
              
            
          
        
          
            
              
                <em>Mosbach, Marius</em>,
              
            
          
        
          
            
              
                
                  Zaitova, Iuliia,
                
              
            
          
        
          
            
              
                
                  Möbius, Bernd,
                
              
            
          
        
          
            
              
                
                  and Klakow, Dietrich
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>Interspeech</em>
      
      
        2021
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/abs/2106.08686" target="_blank">HTML</a>]
    
    
    
    
    
    
      [<a href="https://github.com/uds-lsv" target="_blank">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Several variants of deep neural networks have been successfully employed for building parametric models that project variable-duration spoken word segments onto fixed-size vector representations, or acoustic word embeddings (AWEs). However, it remains unclear to what degree we can rely on the distance in the emerging AWE space as an estimate of word-form similarity. In this paper, we ask: does the distance in the acoustic embedding space correlate with phonological dissimilarity? To answer this question, we empirically investigate the performance of supervised approaches for AWEs with different neural architectures and learning objectives. We train AWE models in controlled settings for two languages (German and Czech) and evaluate the embeddings on two tasks: word discrimination and phonological similarity. Our experiments show that (1) the distance in the embedding space in the best cases only moderately correlates with phonological distance, and (2) improving the performance on the word discrimination task does not necessarily yield models that better reflect word phonological similarity. Our findings highlight the necessity to rethink the current intrinsic evaluations for AWEs.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">COLING 2020</abbr>
      
    
  
  </div>

  <div id="mosbach2020closer" class="col-sm-8">
    
      <span class="title">A Closer Look at Linguistic Knowledge in Masked Language Models: The Case of Relative Clauses in American English</span>
      <span class="author">
        
          
            
              
                <em>Mosbach, Marius</em>,
              
            
          
        
          
            
              
                
                  Degaetano-Ortlieb, Stefania,
                
              
            
          
        
          
            
              
                
                  Krielke, Marie-Pauline,
                
              
            
          
        
          
            
              
                
                  Abdullah, Badr M.,
                
              
            
          
        
          
            
              
                
                  and Klakow, Dietrich
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>COLING</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/abs/2011.00960" target="_blank">HTML</a>]
    
    
    
    
    
    
      [<a href="https://github.com/uds-lsv" target="_blank">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Transformer-based language models achieve high performance on various tasks, but we still lack understanding of the kind of linguistic knowledge they learn and rely on. We evaluate three models (BERT, RoBERTa, and ALBERT), testing their grammatical and semantic knowledge by sentence-level probing, diagnostic cases, and masked prediction tasks. We focus on relative clauses (in American English) as a complex phenomenon needing contextual information and antecedent identification to be resolved. Based on a naturalistic dataset, probing shows that all three models indeed capture linguistic knowledge about grammaticality, achieving high performance. Evaluation on diagnostic cases and masked prediction tasks considering fine-grained linguistic knowledge, however, shows pronounced model-specific weaknesses especially on semantic knowledge, strongly impacting models’ performance. Our results highlight the importance of (a)model comparison in evaluation task and (b) building up claims of model performance and the linguistic knowledge they capture beyond purely probing-based evaluations.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP 2020</abbr>
      
    
  
  </div>

  <div id="mosbach2020stability" class="col-sm-8">
    
      <span class="title">On the Interplay Between Fine-tuning and Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers</span>
      <span class="author">
        
          
            
              
                <em>Mosbach, Marius</em>,
              
            
          
        
          
            
              
                
                  Khokhlova, Anna,
                
              
            
          
        
          
            
              
                
                  Hedderich, Michael A.,
                
              
            
          
        
          
            
              
                
                  and Klakow, Dietrich
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>Findings of EMNLP and BlackboxNLP</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/abs/2010.02616" target="_blank">HTML</a>]
    
    
    
    
    
    
      [<a href="https://github.com/uds-lsv" target="_blank">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Fine-tuning pre-trained contextualized embedding models has become an integral part of the NLP pipeline. At the same time, probing has emerged as a way to investigate the linguistic knowledge captured by pre-trained models. Very little is, however, understood about how fine-tuning affects the representations of pre-trained models and thereby the linguistic knowledge they encode. This paper contributes towards closing this gap. We study three different pre-trained models: BERT, RoBERTa, and ALBERT, and investigate through sentence-level probing how fine-tuning affects their representations. We find that for some probing tasks fine-tuning leads to substantial changes in accuracy, possibly suggesting that fine-tuning introduces or even removes linguistic knowledge from a pre-trained model. These changes, however, vary greatly across different models, fine-tuning and probing tasks. Our analysis reveals that while fine-tuning indeed changes the representations of a pre-trained model and these changes are typically larger for higher layers, only in very few cases, fine-tuning has a positive effect on probing accuracy that is larger than just using the pre-trained model with a strong pooling method. Based on our findings, we argue that both positive and negative effects of fine-tuning on probing require a careful interpretation.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR 2020</abbr>
      
    
  
  </div>

  <div id="mosbach2020stabilitz" class="col-sm-8">
    
      <span class="title">On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines</span>
      <span class="author">
        
          
            
              
                <em>Mosbach, Marius</em>,
              
            
          
        
          
            
              
                
                  Andriushchenko, Maksym,
                
              
            
          
        
          
            
              
                
                  and Klakow, Dietrich
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>ICLR</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/abs/2006.04884" target="_blank">HTML</a>]
    
    
    
    
    
    
      [<a href="https://github.com/uds-lsv/bert-stable-fine-tuning" target="_blank">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Fine-tuning pre-trained transformer-based language models such as BERT has become a common practice dominating leaderboards across various NLP benchmarks. Despite the strong empirical performance of fine-tuned models, fine-tuning is an unstable process: training the same model with multiple random seeds can result in a large variance of the task performance. Previous literature (Devlin et al., 2019; Lee et al., 2020; Dodge et al., 2020) identified two potential reasons for the observed instability: catastrophic forgetting and a small size of the fine-tuning datasets. In this paper, we show that both hypotheses fail to explain the fine-tuning instability. We analyze BERT, RoBERTa, and ALBERT, fine-tuned on three commonly used datasets from the GLUE benchmark and show that the observed instability is caused by optimization difficulties that lead to vanishing gradients. Additionally, we show that the remaining variance of the downstream task performance can be attributed to differences in generalization where fine-tuned models with the same training loss exhibit noticeably different test performance. Based on our analysis, we present a simple but strong baseline that makes fine-tuning BERT-based models significantly more stable than previously proposed approaches.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML 2020</abbr>
      
    
  
  </div>

  <div id="mogadala2020sparse" class="col-sm-8">
    
      <span class="title">Sparse Graph to Sequence Learning for Vision Conditioned Long Textual Sequence Generation</span>
      <span class="author">
        
          
            
              
                
                  Mogadala, Aditya,
                
              
            
          
        
          
            
              
                <em>Mosbach, Marius</em>,
              
            
          
        
          
            
              
                
                  and Klakow, Dietrich
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>ICML Workshop on Bridge Between Perception and Reasoning:
Graph Neural Networks &amp; Beyond</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/abs/2007.06077" target="_blank">HTML</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Generating longer textual sequences when conditioned on the visual information is an interesting problem to explore. The challenge here proliferate over the standard vision conditioned sentence-level generation (e.g., image or video captioning) as it requires to produce a brief and coherent story describing the visual content. In this paper, we mask this Vision-to-Sequence as Graph-to-Sequence learning problem and approach it with the Transformer architecture. To be specific, we introduce Sparse Graph-to-Sequence Transformer (SGST) for encoding the graph and decoding a sequence. The encoder aims to directly encode graph-level semantics, while the decoder is used to generate longer sequences. Experiments conducted with the benchmark image paragraph dataset show that our proposed achieve 13.3% improvement on the CIDEr evaluation measure when comparing to the previous state-of-the-art approach.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NoDaLiDa 2019</abbr>
      
    
  
  </div>

  <div id="bizzoni-etal-2019-steps" class="col-sm-8">
    
      <span class="title">Some steps towards the generation of diachronic WordNets</span>
      <span class="author">
        
          
            
              
                
                  Bizzoni, Yuri,
                
              
            
          
        
          
            
              
                <em>Mosbach, Marius</em>,
              
            
          
        
          
            
              
                
                  Klakow, Dietrich,
                
              
            
          
        
          
            
              
                
                  and Degaetano-Ortlieb, Stefania
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 22nd Nordic Conference on Computational Linguistics</em>
      
      
        2019
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://www.aclweb.org/anthology/W19-6106/" target="_blank">HTML</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>We apply hyperbolic embeddings to trace the dynamics of change of conceptual-semantic relationships in a large diachronic scientific corpus (200 years). Our focus is on emerging scientific fields and the increasingly specialized terminology establishing around them. Reproducing high-quality hierarchical structures such as WordNet on a diachronic scale is a very difficult task. Hyperbolic embeddings can map partial graphs into low dimensional, continuous hierarchical spaces, making more explicit the latent structure of the input. We show that starting from simple lists of word pairs (rather than a list of entities with directional links) it is possible to build diachronic hierarchical semantic spaces which allow us to model a process towards specialization for selected scientific fields.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">RANLP 2019</abbr>
      
    
  
  </div>

  <div id="mosbach-etal-2019-incom" class="col-sm-8">
    
      <span class="title">incom.py - A Toolbox for Calculating Linguistic Distances and Asymmetries between Related Languages</span>
      <span class="author">
        
          
            
              
                <em>Mosbach, Marius</em>,
              
            
          
        
          
            
              
                
                  Stenger, Irina,
                
              
            
          
        
          
            
              
                
                  Avgustinova, Tania,
                
              
            
          
        
          
            
              
                
                  and Klakow, Dietrich
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)</em>
      
      
        2019
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://www.aclweb.org/anthology/R19-1094/" target="_blank">HTML</a>]
    
    
    
    
    
    
      [<a href="https://github.com/uds-lsv/incompy" target="_blank">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Languages may be differently distant from each other and their mutual intelligibility may be asymmetric. In this paper we introduce incom.py, a toolbox for calculating linguistic distances and asymmetries between related languages. incom.py allows linguist experts to quickly and easily perform statistical analyses and compare those with experimental results. We demonstrate the efficacy of incom.py in an incomprehension experiment on two Slavic languages: Bulgarian and Russian. Using incom.py we were able to validate three methods to measure linguistic distances and asymmetries: Levenshtein distance, word adaptation surprisal, and conditional entropy as predictors of success in a reading intercomprehension experiment.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS 2018</abbr>
      
    
  
  </div>

  <div id="mosbach2018logit" class="col-sm-8">
    
      <span class="title">Logit pairing methods can fool gradient-based attacks</span>
      <span class="author">
        
          
            
              
                <em>Mosbach, Marius</em>,
              
            
          
        
          
            
              
                
                  Andriushchenko, Maksym,
                
              
            
          
        
          
            
              
                
                  Trost, Thomas,
                
              
            
          
        
          
            
              
                
                  Hein, Matthias,
                
              
            
          
        
          
            
              
                
                  and Klakow, Dietrich
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>NeurIPS Workshop on Security in Machine Learning</em>
      
      
        2018
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/abs/1810.12042" target="_blank">HTML</a>]
    
    
    
    
    
    
      [<a href="https://github.com/uds-lsv/evaluating-logit-pairing-methods" target="_blank">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Recently, Kannan et al. [2018] proposed several logit regularization methods to improve the adversarial robustness of classifiers. We show that the computationally fast methods they propose - Clean Logit Pairing (CLP) and Logit Squeezing (LSQ) - just make the gradient-based optimization problem of crafting adversarial examples harder without providing actual robustness. We find that Adversarial Logit Pairing (ALP) may indeed provide robustness against adversarial examples, especially when combined with adversarial training, and we examine it in a variety of settings. However, the increase in adversarial accuracy is much smaller than previously claimed. Finally, our results suggest that the evaluation against an iterative PGD attack relies heavily on the parameters used and may result in false conclusions regarding robustness of a model.</p>
    </span>
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    &copy; Copyright 2023 Marius Mosbach.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: December 18, 2023.
    
  </div>
</footer>



  </body>

  <!-- Load Core and Bootstrap JS -->
<script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.0/umd/popper.min.js" integrity="sha256-OH05DFHUWzr725HmuHo3pnuvUUn+TJuj8/Qz9xytFEw=" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/js/mdb.min.js"  integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />


<!-- Load KaTeX -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" integrity="sha256-V8SV2MO1FUb63Bwht5Wx9x6PVHNa02gv8BgH/uH3ung=" crossorigin="anonymous" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js" integrity="sha256-F/Xda58SPdcUCr+xhSGz9MA2zQBPb0ASEYKohl8UCHc=" crossorigin="anonymous"></script>
<script src="/assets/js/katex.js"></script>



<!-- Load Mansory & imagesLoaded -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="" crossorigin="anonymous"></script>
<script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>

<!-- Project Cards Layout -->
<script type="text/javascript">
  // Init Masonry
  var $grid = $('.grid').masonry({
    gutter: 10,
    horizontalOrder: true,
    itemSelector: '.grid-item',
  });
  // layout Masonry after each image loads
  $grid.imagesLoaded().progress( function() {
    $grid.masonry('layout');
  });
</script>







</html>
