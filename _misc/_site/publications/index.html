<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Rati Devidze | publications</title>
<meta name="description" content="Academic website of Marius Mosbach. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/css/mdb.min.css" integrity="sha256-/SwJ2GDcEt5382i8zqDwl36VJGECxEoIcBIuoLmLR4g=" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css"  integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- Open Graph -->


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light bg-white navbar-expand-sm fixed-top">
    <div class="container">
      
      
      
      
      
      <!-- <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Marius</span>   Mosbach
      </a>
      --->
      <div class="navbar-brand title font-weight-lighter">
        <span class="font-weight-bold">Rati</span>   Devidze
      </div>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description">Publications and preprints are in reversed chronological order. For an always up-to-date list check my <a href="https://scholar.google.com/citations?user=y-pLgHgAAAAJ&hl=de&oi=ao"> Google Scholar.</a></p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2024</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AAMAS 2024</abbr>
      
        <!-- <award class="badge">Best paper</award> -->
      
  
  </div>

  <div id="steuer-etal-2023-large" class="col-sm-8">
    
      <span class="title">Informativeness of Reward Functions in Reinforcement Learning</span>
      <span class="author">
              
                <em>Rati Devidze</em>,
                Parameswaran Kamalaruban,
                  and Adish Singla
      </span>

      <span class="periodical">
      
        <em>In International Conference on Autonomous Agents & Multiagent Systems, AAMAS'24 </em>
      
      </span>
    
    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Reward functions are central in specifying the task we want a re-
        inforcement learning agent to perform. Given a task and desired
        optimal behavior, we study the problem of designing informative
        reward functions so that the designed rewards speed up the agent’s
        convergence. In particular, we consider expert-driven reward design
        settings where an expert or teacher seeks to provide informative
        and interpretable rewards to a learning agent. Existing works have
        considered several different reward design formulations; however,
        the key challenge is formulating a reward informativeness criterion
        that adapts w.r.t. the agent’s current policy and can be optimized un-
        der specified structural constraints to obtain interpretable rewards.
        In this paper, we propose a novel reward informativeness crite-
        rion, a quantitative measure that captures how the agent’s current
        policy will improve if it receives rewards from a specific reward
        function. We theoretically showcase the utility of the proposed
        informativeness criterion for adaptively designing rewards for an
        agent. Experimental results on two navigation tasks demonstrate
        the effectiveness of our adaptive reward informativeness criterion.</p>
    </span>
    
  </div>
</div>
</li>
<li>  <h2 class="year">2022</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS 2022</abbr>
      
    
  
  </div>

  <div id="steuer-etal-2023-large" class="col-sm-8">
    
      <span class="title">Exploration-Guided Reward Shaping for Reinforcement Learning under Sparse Rewards</span>
      <span class="author">
        
              
                <em>Rati Devidze</em>,
                  Parameswaran Kamalaruban,
                  and Adish Singla
        
      </span>

      <span class="periodical">
      
        <em>In Conference on Neural Information Processing Systems, NeurIPS'22</em>
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]

      [<a href="https://openreview.net/attachment?id=W7HvKO1erY&name=supplementary_material" target="_blank">HTML</a>]
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>We study the problem of reward shaping to accelerate the training process of
        a reinforcement learning agent. Existing works have considered a number of
        different reward shaping formulations; however, they either require external domain
        knowledge or fail in environments with extremely sparse rewards. In this paper,
        we propose a novel framework, Exploration-Guided Reward Shaping (EXPLORS),
        that operates in a fully self-supervised manner and can accelerate an agent’s
        learning even in sparse-reward environments. The key idea of EXPLORS is to
        learn an intrinsic reward function in combination with exploration-based bonuses
        to maximize the agent’s utility w.r.t. extrinsic rewards. We theoretically showcase
        the usefulness of our reward shaping framework in a special family of MDPs.
        Experimental results on several environments with sparse/noisy reward signals
        demonstrate the effectiveness of EXPLORS.</p>
    </span>
    
  </div>
</div>

</li>
<li>  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS 2021</abbr>
      
    
  
  </div>

  <div id="steuer-etal-2023-large" class="col-sm-8">
    
      <span class="title">Explicable Reward Design for Reinforcement Learning Agents</span>
      <span class="author">
        
              
                <em>Rati Devidze</em>,
                  Goran Radanovic,
                  Parameswaran Kamalaruban,
                  and Adish Singla
        
      </span>

      <span class="periodical">
      
        <em>In Conference on Neural Information Processing Systems, NeurIPS'21</em>
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]

      [<a href="https://machineteaching.mpi-sws.org/files/papers/neurips21_explicable-reward-design.pdf" target="_blank">HTML</a>]
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>We study the design of explicable reward functions for a reinforcement learning
        agent while guaranteeing that an optimal policy induced by the function belongs
        to a set of target policies. By being explicable, we seek to capture two properties:
        (a) informativeness so that the rewards speed up the agent’s convergence, and (b)
        sparseness as a proxy for ease of interpretability of the rewards. The key challenge
        is that higher informativeness typically requires dense rewards for many learning
        tasks, and existing techniques do not allow one to balance these two properties
        appropriately. In this paper, we investigate the problem from the perspective
        of discrete optimization and introduce a novel framework, EXPRD, to design
        explicable reward functions. EXPRD builds upon an informativeness criterion that
        captures the (sub-)optimality of target policies at different time horizons in terms of
        actions taken from any given starting state. We provide a mathematical analysis of
        EXPRD, and show its connections to existing reward design techniques, including
        potential-based reward shaping. Experimental results on two navigation tasks
        demonstrate the effectiveness of EXPRD in designing explicable reward functions.</p>
    </span>
    
  </div>
</div>

</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">JMLR 2021</abbr>
      
    
  
  </div>

  <div id="mosbach-etal-2023-shot" class="col-sm-8">
    
      <span class="title">Policy Teaching in Reinforcement Learning via Environment Poisoning Attacks</span>
      <span class="author">
        
              Amin Rakhsha, 
              Goran Radanovic, 
              <em>Rati Devidze</em>, 
              Xiaojin Zhu, and
              Adish Singla.
        
      </span>

      <span class="periodical">
      
        <em>In Journal of Machine Learning Research, JMLR'21</em>
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    [<a href="https://arxiv.org/abs/2011.10824" target="_blank">HTML</a>]
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>We study a security threat to reinforcement learning where an attacker poisons the learning environment to force the agent into executing a target policy chosen by the attacker. As a victim, we consider RL agents whose objective is to find a policy that maximizes reward in infinite-horizon problem settings. The attacker can manipulate the rewards and the transition dynamics in the learning environment at training-time, and is interested in doing so in a stealthy manner. We propose an optimization framework for finding an optimal stealthy attack for different measures of attack cost. We provide lower/upper bounds on the attack cost, and instantiate our attacks in two settings: (i) an offline setting where the agent is doing planning in the poisoned environment, and (ii) an online setting where the agent is learning a policy with poisoned feedback. Our results show that the attacker can easily succeed in teaching any target policy to the victim under mild conditions and highlight a significant security threat to reinforcement learning agents in practice.</p>
    </span>
    
  </div>
</div>

</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS 2021</abbr>
      
    
  
  </div>

  <div id="mosbach-etal-2023-shot" class="col-sm-8">
    
      <span class="title">Curriculum Design for Teaching via Demonstrations: Theory and Applications</span>
      <span class="author">
        
            Gaurav Yengera, 
            <em>Rati Devidze</em>, 
            Parameswaran Kamalaruban, and
            Adish Singla
        
      </span>

      <span class="periodical">
      
        <em>In Advances in Neural Information Processing Systems, NeurIPS'21</em>
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    [<a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/56c51a39a7c77d8084838cc920585bd0-Paper.pdf" target="_blank">HTML</a>]
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>We consider the problem of teaching via demonstrations in sequential decisionmaking settings. In particular, we study how to design a personalized curriculum
        over demonstrations to speed up the learner’s convergence. We provide a unified
        curriculum strategy for two popular learner models: Maximum Causal Entropy
        Inverse Reinforcement Learning (MaxEnt-IRL) and Cross-Entropy Behavioral
        Cloning (CrossEnt-BC). Our unified strategy induces a ranking over demonstrations based on a notion of difficulty scores computed w.r.t. the teacher’s optimal
        policy and the learner’s current policy. Compared to the state of the art, our
        strategy doesn’t require access to the learner’s internal dynamics and still enjoys
        similar convergence guarantees under mild technical conditions. Furthermore,
        we adapt our curriculum strategy to the setting where no teacher agent is present
        using task-specific difficulty scores. Experiments on a synthetic car driving
        environment and navigation-based environments demonstrate the effectiveness of
        our curriculum strategy</p>
    </span>
    
  </div>
</div>
</li>

<li>  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML 2020</abbr>
      
    
  
  </div>

  <div id="steuer-etal-2023-large" class="col-sm-8">
    
      <span class="title">Policy Teaching via Environment Poisoning: Training-time Adversarial Attacks against Reinforcement Learning</span>
      <span class="author">
        
              Amin Rakhsha, 
              Goran Radanovic, 
              <em>Rati Devidze</em>, 
              Xiaojin Zhu, and
              Adish Singla.
      </span>

      <span class="periodical">
      
        <em>In International Conference on Machine Learning, ICML'20</em>
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]

      [<a href="https://proceedings.mlr.press/v119/rakhsha20a.html" target="_blank">HTML</a>]
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>We study a security threat to reinforcement learning where an attacker poisons the learning environment to force the agent into executing a target policy chosen by the attacker. As a victim, we consider RL agents whose objective is to find a policy that maximizes average reward in undiscounted infinite-horizon problem settings. The attacker can manipulate the rewards or the transition dynamics in the learning environment at training-time and is interested in doing so in a stealthy manner. We propose an optimization framework for finding an \emph{optimal stealthy attack} for different measures of attack cost. We provide sufficient technical conditions under which the attack is feasible and provide lower/upper bounds on the attack cost. We instantiate our attacks in two settings: (i) an \emph{offline} setting where the agent is doing planning in the poisoned environment, and (ii) an \emph{online} setting where the agent is learning a policy using a regret-minimization framework with poisoned feedback. Our results show that the attacker can easily succeed in teaching any target policy to the victim under mild conditions and highlight a significant security threat to reinforcement learning agents in practice.</p>
    </span>
    
  </div>
</div>
</li>

<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IJCAI 2020</abbr>
      
    
  
  </div>

  <div id="mosbach-etal-2023-shot" class="col-sm-8">
    
      <span class="title">Understanding the power and limitations of teaching with imperfect knowledge</span>
      <span class="author">
        
            <em>Rati Devidze</em>, 
            Farnam Mansouri, 
            Luis Haug, 
            Yuxin Chen, and 
            Adish Singla.
      </span>

      <span class="periodical">
      
      
        <em>In International Joint Conference on Artificial Intelligence, IJCAI'20</em>
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]

      [<a href="https://arxiv.org/abs/2003.09712" target="_blank">HTML</a>]
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Machine teaching studies the interaction between a
        teacher and a student/learner where the teacher selects training examples for the learner to learn a specific task. The typical assumption is that the teacher
        has perfect knowledge of the task—this knowledge
        comprises knowing the desired learning target, having the exact task representation used by the learner,
        and knowing the parameters capturing the learning dynamics of the learner. Inspired by real-world
        applications of machine teaching in education, we
        consider the setting where teacher’s knowledge is
        limited and noisy, and the key research question we
        study is the following: When does a teacher succeed or fail in effectively teaching a learner using its
        imperfect knowledge? We answer this question by
        showing connections to how imperfect knowledge
        affects the teacher’s solution of the corresponding
        machine teaching problem when constructing optimal teaching sets. Our results have important implications for designing robust teaching algorithms for
        real-world applications.</p>
    
  </div>
</div>
</li>


</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    &copy; Copyright 2023 Rati Devidze.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: Jan 18, 2024.
    
  </div>
</footer>



  </body>

  <!-- Load Core and Bootstrap JS -->
<script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.0/umd/popper.min.js" integrity="sha256-OH05DFHUWzr725HmuHo3pnuvUUn+TJuj8/Qz9xytFEw=" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/js/mdb.min.js"  integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />


<!-- Load KaTeX -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" integrity="sha256-V8SV2MO1FUb63Bwht5Wx9x6PVHNa02gv8BgH/uH3ung=" crossorigin="anonymous" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js" integrity="sha256-F/Xda58SPdcUCr+xhSGz9MA2zQBPb0ASEYKohl8UCHc=" crossorigin="anonymous"></script>
<script src="/assets/js/katex.js"></script>



<!-- Load Mansory & imagesLoaded -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="" crossorigin="anonymous"></script>
<script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>

<!-- Project Cards Layout -->
<script type="text/javascript">
  // Init Masonry
  var $grid = $('.grid').masonry({
    gutter: 10,
    horizontalOrder: true,
    itemSelector: '.grid-item',
  });
  // layout Masonry after each image loads
  $grid.imagesLoaded().progress( function() {
    $grid.masonry('layout');
  });
</script>







</html>
